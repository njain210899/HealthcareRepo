{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e734db80-2a69-45bc-a326-faea1322d1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../landing-layer/01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe546dba-a678-4fa8-93fe-0753770eb19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = config()\n",
    "df = config.create_config_df()\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e60d0b-eeb6-4383-9e71-0e517e243c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text('hospital_name','hos-a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a7d326-2e57-432c-a161-aef8b1940570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hospital = dbutils.widgets.get('hospital_name')\n",
    "print(hospital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581dc74e-508d-4e24-b6ad-b3ba51f83db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class copy_sql_bronze():\n",
    "    def __init__(self):\n",
    "        self.number_of_rows_copied = 0\n",
    "    def full_data(self,datasource,tablename):\n",
    "        print(tablename)\n",
    "        temp_df = spark.read.format('csv')\\\n",
    "            .option('header','true')\\\n",
    "            .option('inferschema','true')\\\n",
    "            .option('path',config.get_base_dir()+'/'+tablename+'.csv')\\\n",
    "            .load()\n",
    "        print(temp_df.columns)\n",
    "        self.number_of_rows_copied = temp_df.count()\n",
    "        temp_df.write.format('parquet')\\\n",
    "            .mode('overwrite')\\\n",
    "            .save(config.get_bronze_volume_dir()+'/'+tablename+'/'+datasource)\n",
    "        print('Full Load Completed...')\n",
    "\n",
    "    def incremental_data(self,tablename,watermarkcolumnname):\n",
    "        from pyspark.sql.functions import cast\n",
    "        last_insertion_date = spark.sql(f'''\n",
    "                        SELECT coalesce(MAX(loaddate),'1999-12-31') from healthcare_proj_privatews.audit.load_logs where tablename='{tablename}'  ''').collect()[0][0]\n",
    "        print(last_insertion_date)\n",
    "        temp_df = spark.read.format('csv')\\\n",
    "            .option('header','true')\\\n",
    "            .option('mode','PERMISSIVE')\\\n",
    "            .option('inferschema','true')\\\n",
    "            .option('path',config.get_base_dir()+'/'+tablename+'.csv')\\\n",
    "            .load()\n",
    "        \n",
    "        temp_df = temp_df.filter(col(watermarkcolumnname) >=last_insertion_date)\n",
    "        self.number_of_rows_copied = temp_df.count()\n",
    "        temp_df.write.format('parquet')\\\n",
    "            .mode('append')\\\n",
    "            .save(config.get_bronze_volume_dir()+'/'+tablename)\n",
    "        print('Incremental Load Completed...')\n",
    "\n",
    "\n",
    "    def insert_audit_logs(self,datasource,tablename,watermarkcolumnname):\n",
    "        \n",
    "        from pyspark.sql.functions import current_timestamp\n",
    "        from datetime import datetime\n",
    "        current_time = datetime.now()\n",
    "        # print(datasource,tablename,watermarkcolumnname,current_time)\n",
    "        spark.sql('CREATE SCHEMA IF NOT EXISTS healthcare_proj_privatews.audit');\n",
    "        spark.sql('''CREATE TABLE IF NOT EXISTS \n",
    "                  healthcare_proj_privatews.audit.load_logs\n",
    "                  (\n",
    "                      data_source STRING,\n",
    "                      tablename   STRING,\n",
    "                      numberofrowscopied BIGINT,\n",
    "                      watermarkcolumnname STRING,\n",
    "                      loaddate TIMESTAMP\n",
    "                  )\n",
    "                  ''');\n",
    "        spark.sql( f'''\n",
    "        INSERT INTO healthcare_proj_privatews.audit.load_logs\n",
    "        VALUES ('{datasource}', '{tablename}', {self.number_of_rows_copied}, '{watermarkcolumnname}', '{current_time}')\n",
    "    ''')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd7b8da-2a78-475e-9ae8-b5e65dc1cb72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "copy_sql_bronze = copy_sql_bronze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72556942-f931-4213-be3f-c3ef0c2f4197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7e000b-a159-46fe-8d61-391f259329b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, array\n",
    "df = df.filter(col('datasource')==hospital)\n",
    "for i in df.select('tablename').collect():\n",
    "    tablename = i[0].split('.')[1]\n",
    "    loadtype = df.filter(col('tablename')==i[0]).select('loadtype').collect()[0][0]\n",
    "    datasource = df.filter(col('tablename')==i[0]).select('datasource').collect()[0][0]\n",
    "    watermark = df.filter(col('tablename')==i[0]).select('watermark').collect()[0][0]\n",
    "    \n",
    "    if loadtype == 'Incremental':\n",
    "        # copy_sql_bronze.incremental_data(tablename,watermark)\n",
    "        copy_sql_bronze.full_data(datasource,tablename)\n",
    "        copy_sql_bronze.insert_audit_logs(datasource,tablename,watermark)\n",
    "\n",
    "    if loadtype == 'Full':\n",
    "        copy_sql_bronze.full_data(datasource,tablename)\n",
    "        copy_sql_bronze.insert_audit_logs(datasource,tablename,watermark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e696b2d-e469-4be5-a9b9-454a6e23cede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4397208688571793,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02-EMR Data to Bronze",
   "widgets": {
    "hospital_name": {
     "currentValue": "hos-b",
     "nuid": "246c274d-580f-4025-929e-f76c9327cc2f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "hos-a",
      "label": null,
      "name": "hospital_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "hos-a",
      "label": null,
      "name": "hospital_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
